# Hadoop-Based-Tool-for-High-Performance-Distributed-Storage-and-Distributed-Processing

Goal of this project is to store and analyze large chunks of unstructured data, while automating the entire process of cluster management, right from the installation of JDK and Hadoop,configuration of Hadoop throughout the network of systems ,activation of HDFS and MapReduce to facilitating storage of data on HDFS and processing MapReduce jobs. Keeping an end user’s perspective in mind, the project was developed with three modes of operation, A ManualMode, An Automatic Mode, and a Container-based Docker System Mode. A Manual mode enables the client to have a provision to manually select the IP addresses over a network to be selected as a Name Node, a Data Node,a Task Tracker, and Job Tracker and has complete customization rights according to their needs.In an automatic mode, the project automatically configures the available IP addresses over a network, to run Hadoop jobs. In a docker mode, the tool uses docker images instead of using external IP’s, this mode is the fastest and hundreds of docker images can be launched to form the cluster simultaneously, in just a matter of seconds. The code was written in python and a GUI was integrated to make the project look presentable to the end-user.
